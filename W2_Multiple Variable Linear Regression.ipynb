{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Variable Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=2)  # reduced display precision on numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2104    5    1   45]\n",
      " [1416    3    2   40]\n",
      " [ 852    2    1   35]]\n",
      "[460 232 178]\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])\n",
    "print(x_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_initial shape: (4,), b_initial type: <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "b_initial = 785.18113678\n",
    "w_initial = np.array([0.3913, 18.75377, -53.36, -26.42132])\n",
    "print(f\"w_initial shape: {w_initial.shape}, b_initial type: {type(b_initial)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Prediction element by element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b):\n",
    "    n = x.shape[0]\n",
    "    p = 0\n",
    "    for i in range(n):\n",
    "        p_i = x[i] * w[i]\n",
    "        p = p_i + p\n",
    "    p = p + b\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2104    5    1   45]\n",
      "459.92578677999995\n"
     ]
    }
   ],
   "source": [
    "x_vec = x_train[0, :]\n",
    "print(x_vec)\n",
    "f_wb = predict(x_vec, w_initial, b_initial)\n",
    "print(f_wb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `np.dot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459.92578677999995\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def predict_single_vector(x, w, b):\n",
    "    p = np.dot(x, w) + b\n",
    "    return p\n",
    "\n",
    "pred_dot = predict_single_vector(x_vec, w_initial, b_initial)\n",
    "print(pred_dot)\n",
    "print(f_wb == pred_dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cost with Multiple Variables\n",
    "\n",
    "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n",
    "\n",
    "\n",
    "In contrast to previous labs, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0014764204550570398\n"
     ]
    }
   ],
   "source": [
    "def compute_cost(x, y, w, b):\n",
    "    m = x.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        f_wb = np.dot(x[i], w) + b\n",
    "        cost = cost + (f_wb - y[i]) ** 2\n",
    "    cost = cost / (2 * m)\n",
    "    return cost\n",
    "\n",
    "\n",
    "cost = compute_cost(x_train, y_train, w_initial, b_initial)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent With Multiple Variables\n",
    "Gradient descent for multiple variables:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "* m is the number of training examples in the data set\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.39e+01 -1.93e-01 -6.77e-02 -2.12e+00] -0.05122988666679854\n"
     ]
    }
   ],
   "source": [
    "def compute_gradient(x, y, w, b):\n",
    "    m, n = x.shape\n",
    "    dj_dw = np.zeros((n, ))\n",
    "    dj_db = 0.0\n",
    "    for i in range(m):\n",
    "        error = (np.dot(x[i], w) + b) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + error * x[i, j]\n",
    "        dj_db = dj_db + error\n",
    "    dj_db = dj_db / m\n",
    "    dj_dw = dj_dw / m\n",
    "    return dj_dw, dj_db\n",
    "temp_dj_dw, temp_dj_db = compute_gradient(x_train, y_train, w_initial, b_initial)\n",
    "print(temp_dj_dw, temp_dj_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent with Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_init, b_init, cost_fuction, gradient_function, alpha, num_iter):\n",
    "    J_history = []\n",
    "    w = w_init\n",
    "    b = b_init\n",
    "    for i in range(num_iter):\n",
    "        dj_dw, dj_db = gradient_function(x, y, w, b)\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "\n",
    "        J_history.append(cost_fuction(x, y, w, b))\n",
    "        if (i % 100) == 0:\n",
    "            print(f\"Iteration: {i}; Cost: {J_history[-1]}\")\n",
    "    \n",
    "    return w, b, J_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0; Cost: 2529.4629522316304\n",
      "Iteration: 100; Cost: 695.990315835203\n",
      "Iteration: 200; Cost: 694.9206979323061\n",
      "Iteration: 300; Cost: 693.8604297851192\n",
      "Iteration: 400; Cost: 692.8094286135915\n",
      "Iteration: 500; Cost: 691.7676123706057\n",
      "Iteration: 600; Cost: 690.7348997354997\n",
      "Iteration: 700; Cost: 689.711210107616\n",
      "Iteration: 800; Cost: 688.6964635999458\n",
      "Iteration: 900; Cost: 687.6905810327947\n",
      "Iteration: 1000; Cost: 686.6934839275277\n",
      "Iteration: 1100; Cost: 685.705094500366\n",
      "Iteration: 1200; Cost: 684.7253356562205\n",
      "Iteration: 1300; Cost: 683.754130982616\n",
      "Iteration: 1400; Cost: 682.791404743621\n",
      "Iteration: 1500; Cost: 681.8370818738819\n",
      "Iteration: 1600; Cost: 680.8910879726681\n",
      "Iteration: 1700; Cost: 679.9533492980014\n",
      "Iteration: 1800; Cost: 679.0237927608082\n",
      "Iteration: 1900; Cost: 678.1023459191534\n",
      "Prediction: 426.7541288701386 and target value: 460\n",
      "Prediction: 285.8740600118109 and target value: 232\n",
      "Prediction: 170.55111419661225 and target value: 178\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros_like(w_initial)\n",
    "initial_b = 0.0\n",
    "\n",
    "iteration = 2000\n",
    "alpha = 5.0e-7\n",
    "\n",
    "w_final, b_final, J_history = gradient_descent(x_train, y_train, initial_w, initial_b, cost_fuction = compute_cost,\n",
    "                                               gradient_function = compute_gradient, alpha=alpha, num_iter=iteration)\n",
    "M = x_train.shape[0]\n",
    "for i in range(M):\n",
    "    print(f'Prediction: {np.dot(x_train[i], w_final) + b_final} and target value: {y_train[i]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
